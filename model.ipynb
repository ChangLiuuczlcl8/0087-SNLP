{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"m7bqMdEx_PgF","colab_type":"code","outputId":"b9b66256-1f94-4f8b-b9b6-cdd166e5720c","executionInfo":{"status":"ok","timestamp":1552790792288,"user_tz":0,"elapsed":3530,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"zRqPJ2qBUYK2","colab_type":"code","outputId":"295bcefb-5fdf-4ae7-ea3d-ed0521ff06d9","executionInfo":{"status":"ok","timestamp":1552790802828,"user_tz":0,"elapsed":14049,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","from __future__ import division\n","\n","filename = '/content/gdrive/My Drive/MSc ML/0087/glove.6B.50d.txt'\n","def loadGloVe(filename):\n","    vocab = []\n","    embd = []\n","    file = open(filename,'r')\n","    for line in file.readlines():\n","        row = line.strip().split(' ')\n","        vocab.append(row[0])\n","        embd.append(row[1:])\n","    print('Loaded GloVe!')\n","    file.close()\n","    return vocab,embd\n","vocab,embd = loadGloVe(filename)\n","\n","embedding = np.asarray(embd)\n","embedding = embedding.astype(np.float32)\n","\n","word_vec_dim = len(embedding[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded GloVe!\n"],"name":"stdout"}]},{"metadata":{"id":"iwRIz79iU_wn","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/vocab_limit', 'rb') as fp:\n","    vocab_limit = pickle.load(fp)\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/embd_limit', 'rb') as fp:\n","    embd_limit = pickle.load(fp)\n","    \n","with open ('/content/gdrive/My Drive/MSc ML/0087/vec_summaries_test', 'rb') as fp:\n","    vec_summaries_test = pickle.load(fp)\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/vec_texts_test', 'rb') as fp:\n","    vec_texts_test = pickle.load(fp)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"69w3bu7KfieP","colab_type":"code","colab":{}},"cell_type":"code","source":["vocab_limit.append('<SOS>')\n","embd_limit.append(np.zeros((word_vec_dim),dtype=np.float32))\n","\n","SOS = embd_limit[vocab_limit.index('<SOS>')]\n","\n","np_embd_limit = np.asarray(embd_limit,dtype=np.float32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hvX8eq4tI2mM","colab_type":"code","outputId":"523ff7d1-36ab-4ce9-cb43-3e7a2f9d9621","executionInfo":{"status":"ok","timestamp":1552792026515,"user_tz":0,"elapsed":3026,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.reset_default_graph()\n","\n","hidden_size = 500\n","learning_rate = 0.003\n","K = 5\n","vocab_len = len(vocab_limit)\n","training_iters = 5 \n","D = 10 \n","\n","wf_f = tf.get_variable(\"wf_f\", shape=[word_vec_dim,hidden_size])\n","uf_f = tf.get_variable(\"uf_f\", shape=[hidden_size, hidden_size])\n","bf_f = tf.get_variable(\"bf_f\", shape=[1,hidden_size])\n","wi_f = tf.get_variable(\"wi_f\", shape=[word_vec_dim,hidden_size])\n","ui_f = tf.get_variable(\"ui_f\", shape=[hidden_size, hidden_size])\n","bi_f = tf.get_variable(\"bi_f\", shape=[1,hidden_size])\n","wo_f = tf.get_variable(\"wo_f\", shape=[word_vec_dim,hidden_size])\n","uo_f = tf.get_variable(\"uo_f\", shape=[hidden_size, hidden_size])\n","bo_f = tf.get_variable(\"bo_f\", shape=[1,hidden_size])\n","wc_f = tf.get_variable(\"wc_f\", shape=[word_vec_dim,hidden_size])\n","uc_f = tf.get_variable(\"uc_f\", shape=[hidden_size, hidden_size])\n","bc_f = tf.get_variable(\"bc_f\", shape=[1,hidden_size])\n","Wattention_f = tf.get_variable(\"Wattention_f\", shape = [K,1])\n","\n","wf_b = tf.get_variable(\"wf_b\", shape=[word_vec_dim,hidden_size])\n","uf_b = tf.get_variable(\"uf_b\", shape=[hidden_size, hidden_size])\n","bf_b = tf.get_variable(\"bf_b\", shape=[1,hidden_size])\n","wi_b = tf.get_variable(\"wi_b\", shape=[word_vec_dim,hidden_size])\n","ui_b = tf.get_variable(\"ui_b\", shape=[hidden_size, hidden_size])\n","bi_b = tf.get_variable(\"bi_b\", shape=[1,hidden_size])\n","wo_b = tf.get_variable(\"wo_b\", shape=[word_vec_dim,hidden_size])\n","uo_b = tf.get_variable(\"uo_b\", shape=[hidden_size, hidden_size])\n","bo_b = tf.get_variable(\"bo_b\", shape=[1,hidden_size])\n","wc_b = tf.get_variable(\"wc_b\", shape=[word_vec_dim,hidden_size])\n","uc_b = tf.get_variable(\"uc_b\", shape=[hidden_size, hidden_size])\n","bc_b = tf.get_variable(\"bc_b\", shape=[1,hidden_size])\n","Wattention_b = tf.get_variable(\"Wattention_b\", shape = [K,1])\n","\n","Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,50])\n","Vp = tf.get_variable(\"Vp\", shape=[50,1])\n","Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size])\n","Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,2*hidden_size])\n","\n","Ws = tf.get_variable(\"Ws\", shape=[2*hidden_size,vocab_len])\n","\n","wf_d = tf.get_variable(\"wf_d\", shape=[word_vec_dim,2*hidden_size])\n","uf_d = tf.get_variable(\"uf_d\", shape = [2*hidden_size, 2*hidden_size])\n","bf_d = tf.get_variable(\"bf_d\", shape = [1,2*hidden_size])\n","wi_d = tf.get_variable(\"wi_d\", shape = [word_vec_dim,2*hidden_size])\n","ui_d = tf.get_variable(\"ui_d\", shape = [2*hidden_size, 2*hidden_size])\n","bi_d = tf.get_variable(\"bi_d\", shape = [1,2*hidden_size])\n","wo_d = tf.get_variable(\"wo_d\", shape = [word_vec_dim,2*hidden_size])\n","uo_d = tf.get_variable(\"uo_d\", shape = [2*hidden_size, 2*hidden_size])\n","bo_d = tf.get_variable(\"bo_d\", shape = [1,2*hidden_size])\n","wc_d = tf.get_variable(\"wc_d\", shape = [word_vec_dim,2*hidden_size])\n","uc_d = tf.get_variable(\"uc_d\", shape = [2*hidden_size, 2*hidden_size])\n","bc_d = tf.get_variable(\"bc_d\", shape = [1,2*hidden_size])\n","\n","Wattention_d = tf.get_variable(\"Wattention_d\", shape = [K,1])\n","\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    saver = tf.train.import_meta_graph(\"/content/gdrive/My Drive/MSc ML/0087/model.ckpt.meta\")\n","    saver.restore(sess,\"/content/gdrive/My Drive/MSc ML/0087/model.ckpt\")\n","    print(Ws)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/MSc ML/0087/model.ckpt\n","<tf.Variable 'Ws:0' shape=(1000, 47289) dtype=float32_ref>\n"],"name":"stdout"}]},{"metadata":{"id":"dFrGOUgpgFsX","colab_type":"code","colab":{}},"cell_type":"code","source":["def np_nearest_neighbour(x):\n","    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n","        \n","    xdoty = np.multiply(embedding,x)\n","    xdoty = np.sum(xdoty,1)\n","    xlen = np.square(x)\n","    xlen = np.sum(xlen,0)\n","    xlen = np.sqrt(xlen)\n","    ylen = np.square(embedding)\n","    ylen = np.sum(ylen,1)\n","    ylen = np.sqrt(ylen)\n","    xlenylen = np.multiply(xlen,ylen)\n","    cosine_similarities = np.divide(xdoty,xlenylen)\n","\n","    return embedding[np.argmax(cosine_similarities)]\n","\n","\n","def word2vec(word):  # converts a given word into its vector representation\n","    if word in vocab:\n","        return embedding[vocab.index(word)]\n","    else:\n","        return embedding[vocab.index('unk')]\n","\n","def vec2word(vec):   # converts a given vector representation into the represented word \n","    for x in xrange(0, len(embedding)):\n","        if np.array_equal(embedding[x],np.asarray(vec)):\n","            return vocab[x]\n","    return vec2word(np_nearest_neighbour(np.asarray(vec)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I4OXtVgJ_B54","colab_type":"code","colab":{}},"cell_type":"code","source":["def transform_out(output_text):\n","    output_len = len(output_text)\n","    transformed_output = np.zeros([output_len],dtype=np.int32)\n","    for i in xrange(0,output_len):\n","        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n","    return transformed_output \n","  \n","global graph\n","graph = tf.get_default_graph()\n","  \n","tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n","tf_seq_len = tf.placeholder(tf.int32)\n","tf_summary = tf.placeholder(tf.int32,[None])\n","tf_output_len = tf.placeholder(tf.int32)\n","\n","def forward_encoder(inp,hidden,cell,\n","                    wf,uf,bf,\n","                    wi,ui,bi,\n","                    wo,uo,bo,\n","                    wc,uc,bc,\n","                    Wattention,seq_len,inp_dim):\n","\n","    Wattention = tf.nn.softmax(Wattention,0)\n","    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n","    \n","    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n","    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n","    \n","    i=0\n","    j=K\n","    \n","    def cond(i,j,hidden,cell,hidden_forward,hidden_residuals):\n","        return i < seq_len\n","    \n","    def body(i,j,hidden,cell,hidden_forward,hidden_residuals):\n","        \n","        x = tf.reshape(inp[i],[1,inp_dim])\n","        \n","        hidden_residuals_stack = hidden_residuals.stack()\n","        \n","        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n","        RRA = tf.reshape(RRA,[1,hidden_size])\n","        \n","        # LSTM with RRA\n","        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n","        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n","        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n","        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n","        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n","        \n","        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n","                                   lambda: hidden_residuals,\n","                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n","\n","        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n","        \n","        return i+1,j+1,hidden,cell,hidden_forward,hidden_residuals\n","    \n","    _,_,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_forward,hidden_residuals])\n","    \n","    hidden_residuals.close().mark_used()\n","    \n","    return hidden_forward.stack()\n","        \n","    \n","def backward_encoder(inp,hidden,cell,\n","                     wf,uf,bf,\n","                     wi,ui,bi,\n","                     wo,uo,bo,\n","                     wc,uc,bc,\n","                     Wattention,seq_len,inp_dim):\n","    \n","    Wattention = tf.nn.softmax(Wattention,0)\n","    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n","    \n","    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n","    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n","    \n","    i=seq_len-1\n","    j=K\n","    \n","    def cond(i,j,hidden,cell,hidden_backward,hidden_residuals):\n","        return i > -1\n","    \n","    def body(i,j,hidden,cell,hidden_backward,hidden_residuals):\n","        \n","        x = tf.reshape(inp[i],[1,inp_dim])\n","        \n","        hidden_residuals_stack = hidden_residuals.stack()\n","        \n","        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n","        RRA = tf.reshape(RRA,[1,hidden_size])\n","        \n","        # LSTM with RRA\n","        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n","        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n","        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n","        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n","        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n","\n","        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n","                                   lambda: hidden_residuals,\n","                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n","        \n","        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n","        \n","        return i-1,j+1,hidden,cell,hidden_backward,hidden_residuals\n","    \n","    _,_,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_backward,hidden_residuals])\n","\n","    hidden_residuals.close().mark_used()\n","    \n","    return hidden_backward.stack()\n","        \n","    \n","def decoder(x,hidden,cell,\n","            wf,uf,bf,\n","            wi,ui,bi,\n","            wo,uo,bo,\n","            wc,uc,bc,RRA):\n","    \n","    # LSTM with RRA\n","    fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n","    ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n","    og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n","    cell_next = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n","    hidden_next = tf.multiply(og,tf.tanh(cell+RRA))\n","    \n","    return hidden_next,cell_next\n","  \n","  \n","def score(hs,ht,Wa,seq_len):\n","    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n","\n","def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n","   \n","    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n","    \n","    positions = tf.cast(tf_seq_len-1-2*D,dtype=tf.float32)\n","    \n","    sigmoid_multiplier = tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp))\n","    sigmoid_multiplier = tf.reshape(sigmoid_multiplier,[])\n","    \n","    pt_float = positions*sigmoid_multiplier\n","    \n","    pt = tf.cast(pt_float,tf.int32)\n","    pt = pt+D #center to window\n","    \n","    sigma = tf.constant(D/2,dtype=tf.float32)\n","    \n","    i = 0\n","    pos = pt - D\n","    \n","    def cond(i,pos,pd):\n","        \n","        return i < (2*D+1)\n","                      \n","    def body(i,pos,pd):\n","            \n","        pd = pd.write(i,tf.exp(-(tf.cast(tf.square(pos-pt),tf.float32)\n","                                 /tf.cast(2*tf.square(sigma),tf.float32))))\n","            \n","        return i+1,pos+1,pd\n","                      \n","    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n","    \n","    local_hs = hs[(pt-D):(pt+D+1)]\n","    \n","    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n","    \n","    pd=pd.stack()\n","    \n","    G = tf.multiply(normalized_scores,pd)\n","    G = tf.reshape(G,[2*D+1,1])\n","    \n","    return G,pt\n","\n","  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w8f1snRXYGM1","colab_type":"code","colab":{}},"cell_type":"code","source":["def model(tf_text,tf_seq_len,tf_output_len):\n","    \n","    #PARAMETERS\n","    \n","    #1.1 FORWARD ENCODER PARAMETERS\n","    \n","    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    cell_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    \n","    #1.2 BACKWARD ENCODER PARAMETERS\n","    \n","    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    cell_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    \n","    #3 DECODER PARAMETERS\n","    cell_d = tf.zeros([1,2*hidden_size],dtype=tf.float32)\n","    \n","    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n","    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n","    \n","    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n","    \n","    #BI-DIRECTIONAL LSTM\n","                               \n","    hidden_forward = forward_encoder(tf_text,\n","                                     initial_hidden_f,cell_f,\n","                                     wf_f,uf_f,bf_f,\n","                                     wi_f,ui_f,bi_f,\n","                                     wo_f,uo_f,bo_f,\n","                                     wc_f,uc_f,bc_f,\n","                                     Wattention_f,\n","                                     tf_seq_len,\n","                                     word_vec_dim)\n","    \n","    hidden_backward = backward_encoder(tf_text,\n","                                     initial_hidden_b,cell_b,\n","                                     wf_b,uf_b,bf_b,\n","                                     wi_b,ui_b,bi_b,\n","                                     wo_b,uo_b,bo_b,\n","                                     wc_b,uc_b,bc_b,\n","                                     Wattention_b,\n","                                     tf_seq_len,\n","                                     word_vec_dim)\n","    \n","    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n","    \n","    \n","    #ATTENTION MECHANISM AND DECODER\n","    \n","    decoded_hidden = encoded_hidden[0]\n","    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n","    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n","    tf_embd_limit = tf.convert_to_tensor(np_embd_limit)\n","    \n","    y = tf.convert_to_tensor(SOS) #inital decoder token <SOS> vector\n","    y = tf.reshape(y,[1,word_vec_dim])\n","    \n","    j=K\n","    \n","    hidden_residuals_stack = hidden_residuals_d.stack()\n","    \n","    RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n","    RRA = tf.reshape(RRA,[1,2*hidden_size])\n","    \n","    decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n","                                  wf_d,uf_d,bf_d,\n","                                  wi_d,ui_d,bf_d,\n","                                  wo_d,uo_d,bf_d,\n","                                  wc_d,uc_d,bc_d,\n","                                  RRA)\n","    decoded_hidden = decoded_hidden_next\n","    \n","    hidden_residuals_d = hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size]))\n","    \n","    j=j+1\n","                           \n","    i=0\n","    \n","    def attention_decoder_cond(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n","        return i < tf_output_len\n","    \n","    def attention_decoder_body(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):     \n","        #LOCAL ATTENTION\n","        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n","        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n","        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n","        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n","        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n","        \n","        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n","        \n","        #DECODER\n","        \n","        y = tf.matmul(attended_hidden,Ws)\n","        \n","        output = output.write(i,tf.reshape(y,[vocab_len]))\n","        #Save probability distribution as output\n","        \n","        y = tf.nn.softmax(y)\n","        \n","        y_index = tf.cast(tf.argmax(tf.reshape(y,[vocab_len])),tf.int32)\n","        y = tf_embd_limit[y_index]\n","        y = tf.reshape(y,[1,word_vec_dim])\n","        \n","        #setting next decoder input token as the word_vector of maximum probability \n","        #as found from previous attention-decoder output.\n","        hidden_residuals_stack = hidden_residuals_d.stack()\n","        \n","        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n","        RRA = tf.reshape(RRA,[1,2*hidden_size])\n","        \n","        decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n","                                  wf_d,uf_d,bf_d,\n","                                  wi_d,ui_d,bf_d,\n","                                  wo_d,uo_d,bf_d,\n","                                  wc_d,uc_d,bc_d,\n","                                  RRA)\n","        \n","        decoded_hidden = decoded_hidden_next\n","        \n","        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K+1), #(+1 for <SOS>)\n","                                   lambda: hidden_residuals_d,\n","                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n","        \n","        return i+1,j+1,decoded_hidden,cell_d,hidden_residuals_d,output\n","    \n","    i,j,decoded_hidden,cell_d,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n","                                            attention_decoder_body,\n","                                            [i,j,decoded_hidden,cell_d,hidden_residuals_d,output])\n","    hidden_residuals_d.close().mark_used()\n","    \n","    output = output.stack()\n","    \n","    return output\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"IlmSCewCY7Vx","colab_type":"code","colab":{}},"cell_type":"code","source":["output = model(tf_text,tf_seq_len,tf_output_len)\n","\n","#OPTIMIZER\n","\n","cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n","\n","#PREDICTION\n","\n","pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n","\n","i=0\n","\n","def cond_pred(i,pred):\n","    return i<tf_output_len\n","def body_pred(i,pred):\n","    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n","    return i+1,pred\n","\n","i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n","\n","prediction = pred.stack()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oXfXzMFfaoDW","colab_type":"code","colab":{}},"cell_type":"code","source":["import string\n","from __future__ import print_function\n","\n","init = tf.global_variables_initializer()\n","\n","\n","with tf.Session() as sess: \n","    step = 0   \n","    loss_list=[]\n","    acc_list=[]\n","    val_loss_list=[]\n","    val_acc_list=[]\n","    best_val_acc=0\n","    display_step = 1\n","    \n","\n","\n","    total_loss=0\n","    total_acc=0\n","    total_val_loss = 0\n","    total_val_acc = 0\n","\n","    for i in xrange(0,len(vec_texts_test)):\n","\n","        train_out = transform_out(vec_summaries_test[i][0:len(vec_summaries_test[i])-1])\n","        \n","        if i%display_step==0:\n","            print(\"\\nIteration: \"+str(i))\n","            print(\"Training input sequence length: \"+str(len(vec_texts_test[i])))\n","            print(\"Training target outputs sequence length: \"+str(len(train_out)))\n","\n","            print(\"\\nTEXT:\")\n","            flag = 0\n","            for vec in vec_texts_test[i]:\n","                if vec2word(vec) in string.punctuation or flag==0:\n","                    print(str(vec2word(vec)),end='')\n","                else:\n","                    print((\" \"+str(vec2word(vec))),end='')\n","                flag=1\n","\n","            print(\"\\n\")\n","        \n","        loss,pred = sess.run([cost,prediction],feed_dict={tf_text: vec_texts_test[i], tf_seq_len: len(vec_texts_test[i]), tf_summary: train_out, tf_output_len: len(train_out)})\n","\n","        if i%display_step==0:\n","\n","\n","            print(\"\\nPREDICTED SUMMARY:\\n\")\n","            flag = 0\n","            for index in pred:\n","                #if int(index)!=vocab_limit.index('eos'):\n","                if vocab_limit[int(index)] in string.punctuation or flag==0:\n","                    print(str(vocab_limit[int(index)]),end='')\n","                else:\n","                    print(\" \"+str(vocab_limit[int(index)]),end='')\n","                flag=1\n","            print(\"\\n\")\n","\n","            print(\"ACTUAL SUMMARY:\\n\")\n","            flag = 0\n","            for vec in vec_summaries_test[i]:\n","                if vec2word(vec)!='eos':\n","                    if vec2word(vec) in string.punctuation or flag==0:\n","                        print(str(vec2word(vec)),end='')\n","                    else:\n","                        print((\" \"+str(vec2word(vec))),end='')\n","                flag=1\n","\n","            print(\"\\n\")\n","            print(\"loss=\"+str(loss))\n","\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"TE7hW90QKTKm","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}