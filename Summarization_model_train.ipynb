{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Summarization_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"mIGmifSwPdP8","colab_type":"code","outputId":"7d8f86b0-9eb0-4a05-810b-e39dc9432740","executionInfo":{"status":"ok","timestamp":1552840093139,"user_tz":0,"elapsed":22452,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"BnxzO_VH7J4-","colab_type":"code","outputId":"90a8deb6-b5f1-4eb1-bac4-7aa385bbc620","executionInfo":{"status":"ok","timestamp":1552840103760,"user_tz":0,"elapsed":33057,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","from __future__ import division\n","\n","filename = '/content/gdrive/My Drive/MSc ML/0087/glove.6B.50d.txt'\n","def loadGloVe(filename):\n","    vocab = []\n","    embd = []\n","    file = open(filename,'r')\n","    for line in file.readlines():\n","        row = line.strip().split(' ')\n","        vocab.append(row[0])\n","        embd.append(row[1:])\n","    print('Loaded GloVe!')\n","    file.close()\n","    return vocab,embd\n","vocab,embd = loadGloVe(filename)\n","\n","embedding = np.asarray(embd)\n","embedding = embedding.astype(np.float32)\n","\n","word_vec_dim = len(embedding[0])\n","#Pre-trained GloVe embedding"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded GloVe!\n"],"name":"stdout"}]},{"metadata":{"id":"bXODfJQi7J5C","colab_type":"code","colab":{}},"cell_type":"code","source":["def np_nearest_neighbour(x):\n","    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n","        \n","    xdoty = np.multiply(embedding,x)\n","    xdoty = np.sum(xdoty,1)\n","    xlen = np.square(x)\n","    xlen = np.sum(xlen,0)\n","    xlen = np.sqrt(xlen)\n","    ylen = np.square(embedding)\n","    ylen = np.sum(ylen,1)\n","    ylen = np.sqrt(ylen)\n","    xlenylen = np.multiply(xlen,ylen)\n","    cosine_similarities = np.divide(xdoty,xlenylen)\n","\n","    return embedding[np.argmax(cosine_similarities)]\n","\n","\n","def word2vec(word):  # converts a given word into its vector representation\n","    if word in vocab:\n","        return embedding[vocab.index(word)]\n","    else:\n","        return embedding[vocab.index('unk')]\n","\n","def vec2word(vec):   # converts a given vector representation into the represented word \n","    for x in xrange(0, len(embedding)):\n","        if np.array_equal(embedding[x],np.asarray(vec)):\n","            return vocab[x]\n","    return vec2word(np_nearest_neighbour(np.asarray(vec)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"whx3W1Gq7J5G","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/vec_summaries', 'rb') as fp:\n","    vec_summaries = pickle.load(fp)\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/vec_texts', 'rb') as fp:\n","    vec_texts = pickle.load(fp)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"sHYNLplo-J0q","colab_type":"code","colab":{}},"cell_type":"code","source":["with open ('/content/gdrive/My Drive/MSc ML/0087/vec_summaries_test', 'rb') as fp:\n","    vec_summaries_test = pickle.load(fp)\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/vec_texts_test', 'rb') as fp:\n","    vec_texts_test = pickle.load(fp)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8KCsqM4f7J5K","colab_type":"code","colab":{}},"cell_type":"code","source":["with open ('/content/gdrive/My Drive/MSc ML/0087/vocab_limit', 'rb') as fp:\n","    vocab_limit = pickle.load(fp)\n","\n","with open ('/content/gdrive/My Drive/MSc ML/0087/embd_limit', 'rb') as fp:\n","    embd_limit = pickle.load(fp)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"zgF0uctn7J5O","colab_type":"code","colab":{}},"cell_type":"code","source":["vocab_limit.append('<SOS>')\n","embd_limit.append(np.zeros((word_vec_dim),dtype=np.float32))\n","\n","SOS = embd_limit[vocab_limit.index('<SOS>')]\n","np_embd_limit = np.asarray(embd_limit,dtype=np.float32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"04f3ikDM7J5S","colab_type":"code","colab":{}},"cell_type":"code","source":["#DIAGNOSIS\n","\n","count = 0\n","\n","LEN = 8\n","\n","for summary in vec_summaries:\n","    if len(summary)-1>LEN:\n","        count = count + 1\n","\n","\n","count = 0\n","\n","D = 10 \n","\n","window_size = 2*D+1\n","\n","for text in vec_texts:\n","    if len(text)<window_size+1:\n","        count = count + 1\n","\n","\n","count = 0\n","LEN = 80\n","\n","for text in vec_texts:\n","    if len(text)>LEN:\n","        count = count + 1\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PMHHRT1y7J5X","colab_type":"code","colab":{}},"cell_type":"code","source":["MAX_SUMMARY_LEN = 8\n","MAX_TEXT_LEN = 80\n","\n","#D is a major hyperparameters. Windows size for local attention will be 2*D+1\n","D = 10\n","\n","window_size = 2*D+1\n","\n","#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n","#OR WHOSE TEXT LENGTH IS TOO BIG\n","#OR WHOSE TEXT LENGTH IS SMALLED THAN WINDOW SIZE\n","\n","vec_summaries_reduced = []\n","vec_texts_reduced = []\n","\n","i = 0\n","for summary in vec_summaries:\n","    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])>=window_size and len(vec_texts[i])<=MAX_TEXT_LEN:\n","        vec_summaries_reduced.append(summary)\n","        vec_texts_reduced.append(vec_texts[i])\n","    i=i+1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oCscc1xJ7J5a","colab_type":"code","colab":{}},"cell_type":"code","source":["train_len = int((.85)*len(vec_summaries_reduced))\n","\n","train_texts = vec_texts_reduced[:train_len]\n","train_summaries = vec_summaries_reduced[:train_len]\n","\n","val_texts = vec_texts_reduced[train_len:]\n","val_summaries = vec_summaries_reduced[train_len:]\n","\n","test_texts = vec_texts_test\n","test_summaries = vec_summaries_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yvvOdjip7J5c","colab_type":"code","outputId":"c8c7e3cb-1275-4e22-b5fb-6fbe5dcb4496","executionInfo":{"status":"ok","timestamp":1552840717836,"user_tz":0,"elapsed":456,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(train_len)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["16076\n"],"name":"stdout"}]},{"metadata":{"id":"Az0iqijH7J5f","colab_type":"code","colab":{}},"cell_type":"code","source":["def transform_out(output_text):\n","    output_len = len(output_text)\n","    transformed_output = np.zeros([output_len],dtype=np.int32)\n","    for i in xrange(0,output_len):\n","        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n","    return transformed_output   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"AmiloNJ87J5l","colab_type":"code","colab":{}},"cell_type":"code","source":["#Some MORE hyperparameters and other stuffs\n","\n","hidden_size = 500\n","learning_rate = 0.003\n","K = 5\n","vocab_len = len(vocab_limit)\n","training_iters = 5 "],"execution_count":0,"outputs":[]},{"metadata":{"id":"gIroJ6jZ7J5o","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","#placeholders\n","tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n","tf_seq_len = tf.placeholder(tf.int32)\n","tf_summary = tf.placeholder(tf.int32,[None])\n","tf_output_len = tf.placeholder(tf.int32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1OQ0q9eu7J5q","colab_type":"code","colab":{}},"cell_type":"code","source":["def forward_encoder(inp,hidden,cell,\n","                    wf,uf,bf,\n","                    wi,ui,bi,\n","                    wo,uo,bo,\n","                    wc,uc,bc,\n","                    Wattention,seq_len,inp_dim):\n","\n","    Wattention = tf.nn.softmax(Wattention,0)\n","    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n","    \n","    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n","    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n","    \n","    i=0\n","    j=K\n","    \n","    def cond(i,j,hidden,cell,hidden_forward,hidden_residuals):\n","        return i < seq_len\n","    \n","    def body(i,j,hidden,cell,hidden_forward,hidden_residuals):\n","        \n","        x = tf.reshape(inp[i],[1,inp_dim])\n","        \n","        hidden_residuals_stack = hidden_residuals.stack()\n","        \n","        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n","        RRA = tf.reshape(RRA,[1,hidden_size])\n","        \n","        # LSTM with RRA\n","        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n","        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n","        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n","        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n","        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n","        \n","        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n","                                   lambda: hidden_residuals,\n","                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n","\n","        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n","        \n","        return i+1,j+1,hidden,cell,hidden_forward,hidden_residuals\n","    \n","    _,_,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_forward,hidden_residuals])\n","    \n","    hidden_residuals.close().mark_used()\n","    \n","    return hidden_forward.stack()\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"5zi0Hxcg7J5s","colab_type":"code","colab":{}},"cell_type":"code","source":["def backward_encoder(inp,hidden,cell,\n","                     wf,uf,bf,\n","                     wi,ui,bi,\n","                     wo,uo,bo,\n","                     wc,uc,bc,\n","                     Wattention,seq_len,inp_dim):\n","    \n","    Wattention = tf.nn.softmax(Wattention,0)\n","    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n","    \n","    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n","    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n","    \n","    i=seq_len-1\n","    j=K\n","    \n","    def cond(i,j,hidden,cell,hidden_backward,hidden_residuals):\n","        return i > -1\n","    \n","    def body(i,j,hidden,cell,hidden_backward,hidden_residuals):\n","        \n","        x = tf.reshape(inp[i],[1,inp_dim])\n","        \n","        hidden_residuals_stack = hidden_residuals.stack()\n","        \n","        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n","        RRA = tf.reshape(RRA,[1,hidden_size])\n","        \n","        # LSTM with RRA\n","        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n","        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n","        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n","        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n","        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n","\n","        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n","                                   lambda: hidden_residuals,\n","                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n","        \n","        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n","        \n","        return i-1,j+1,hidden,cell,hidden_backward,hidden_residuals\n","    \n","    _,_,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_backward,hidden_residuals])\n","\n","    hidden_residuals.close().mark_used()\n","    \n","    return hidden_backward.stack()\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"xhBWbuGn7J5t","colab_type":"code","colab":{}},"cell_type":"code","source":["def decoder(x,hidden,cell,\n","            wf,uf,bf,\n","            wi,ui,bi,\n","            wo,uo,bo,\n","            wc,uc,bc,RRA):\n","    \n","    # LSTM with RRA\n","    fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n","    ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n","    og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n","    cell_next = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n","    hidden_next = tf.multiply(og,tf.tanh(cell+RRA))\n","    \n","    return hidden_next,cell_next"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ToPWjwmQ7J5v","colab_type":"code","colab":{}},"cell_type":"code","source":["def score(hs,ht,Wa,seq_len):\n","    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n","\n","def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n","   \n","    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n","    \n","    positions = tf.cast(tf_seq_len-1-2*D,dtype=tf.float32)\n","    \n","    sigmoid_multiplier = tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp))\n","    sigmoid_multiplier = tf.reshape(sigmoid_multiplier,[])\n","    \n","    pt_float = positions*sigmoid_multiplier\n","    \n","    pt = tf.cast(pt_float,tf.int32)\n","    pt = pt+D #center to window\n","    \n","    sigma = tf.constant(D/2,dtype=tf.float32)\n","    \n","    i = 0\n","    pos = pt - D\n","    \n","    def cond(i,pos,pd):\n","        \n","        return i < (2*D+1)\n","                      \n","    def body(i,pos,pd):\n","            \n","        pd = pd.write(i,tf.exp(-(tf.cast(tf.square(pos-pt),tf.float32)\n","                                 /tf.cast(2*tf.square(sigma),tf.float32))))\n","            \n","        return i+1,pos+1,pd\n","                      \n","    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n","    \n","    local_hs = hs[(pt-D):(pt+D+1)]\n","    \n","    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n","    \n","    pd=pd.stack()\n","    \n","    G = tf.multiply(normalized_scores,pd)\n","    G = tf.reshape(G,[2*D+1,1])\n","    \n","    return G,pt\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"veHNGAvG7J5y","colab_type":"code","colab":{}},"cell_type":"code","source":["def model(tf_text,tf_seq_len,tf_output_len):\n","    \n","    #PARAMETERS\n","    \n","    #1.1 FORWARD ENCODER PARAMETERS\n","    \n","    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    cell_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    wf_f = tf.get_variable(\"wf_f\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uf_f = tf.get_variable(\"uf_f\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bf_f = tf.get_variable(\"bf_f\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    wi_f = tf.get_variable(\"wi_f\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    ui_f = tf.get_variable(\"ui_f\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bi_f = tf.get_variable(\"bi_f\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    wo_f = tf.get_variable(\"wo_f\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uo_f = tf.get_variable(\"uo_f\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bo_f = tf.get_variable(\"bo_f\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    wc_f = tf.get_variable(\"wc_f\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uc_f = tf.get_variable(\"uc_f\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bc_f = tf.get_variable(\"bc_f\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    Wattention_f = tf.get_variable(\"Wattention_f\", shape = [K,1], initializer = tf.zeros_initializer)\n","                               \n","    #1.2 BACKWARD ENCODER PARAMETERS\n","    \n","    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    cell_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n","    wf_b = tf.get_variable(\"wf_b\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uf_b = tf.get_variable(\"uf_b\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bf_b = tf.get_variable(\"bf_b\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    wi_b = tf.get_variable(\"wi_b\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    ui_b = tf.get_variable(\"ui_b\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bi_b = tf.get_variable(\"bi_b\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    wo_b = tf.get_variable(\"wo_b\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uo_b = tf.get_variable(\"uo_b\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bo_b = tf.get_variable(\"bo_b\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    wc_b = tf.get_variable(\"wc_b\", shape = [word_vec_dim,hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uc_b = tf.get_variable(\"uc_b\", shape = [hidden_size, hidden_size], initializer = tf.initializers.identity)\n","    bc_b = tf.get_variable(\"bc_b\", shape = [1,hidden_size], initializer = tf.zeros_initializer)\n","    Wattention_b = tf.get_variable(\"Wattention_b\", shape = [K,1], initializer = tf.zeros_initializer)\n","    \n","    #2 ATTENTION PARAMETERS\n","    \n","    Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,50], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    Vp = tf.get_variable(\"Vp\", shape=[50,1], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,2*hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    \n","    #3 DECODER PARAMETERS\n","    \n","    Ws = tf.get_variable(\"Ws\", shape=[2*hidden_size,vocab_len], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    \n","    cell_d = tf.zeros([1,2*hidden_size],dtype=tf.float32)\n","    wf_d = tf.get_variable(\"wf_d\", shape=[word_vec_dim,2*hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uf_d = tf.get_variable(\"uf_d\", shape = [2*hidden_size, 2*hidden_size], initializer = tf.initializers.identity)\n","    bf_d = tf.get_variable(\"bf_d\", shape = [1,2*hidden_size], initializer = tf.zeros_initializer)\n","    wi_d = tf.get_variable(\"wi_d\", shape = [word_vec_dim,2*hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    ui_d = tf.get_variable(\"ui_d\", shape = [2*hidden_size, 2*hidden_size], initializer = tf.initializers.identity)\n","    bi_d = tf.get_variable(\"bi_d\", shape = [1,2*hidden_size], initializer = tf.zeros_initializer)\n","    wo_d = tf.get_variable(\"wo_d\", shape = [word_vec_dim,2*hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uo_d = tf.get_variable(\"uo_d\", shape = [2*hidden_size, 2*hidden_size], initializer = tf.initializers.identity)\n","    bo_d = tf.get_variable(\"bo_d\", shape = [1,2*hidden_size], initializer = tf.zeros_initializer)\n","    wc_d = tf.get_variable(\"wc_d\", shape = [word_vec_dim,2*hidden_size], initializer = tf.truncated_normal_initializer(stddev=0.01))\n","    uc_d = tf.get_variable(\"uc_d\", shape = [2*hidden_size, 2*hidden_size], initializer = tf.initializers.identity)\n","    bc_d = tf.get_variable(\"bc_d\", shape = [1,2*hidden_size], initializer = tf.zeros_initializer)\n","    \n","    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n","    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n","    \n","    Wattention_d = tf.get_variable(\"Wattention_d\", shape = [K,1], initializer = tf.zeros_initializer)\n","    \n","    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n","                               \n","    #BI-DIRECTIONAL LSTM\n","                               \n","    hidden_forward = forward_encoder(tf_text,\n","                                     initial_hidden_f,cell_f,\n","                                     wf_f,uf_f,bf_f,\n","                                     wi_f,ui_f,bi_f,\n","                                     wo_f,uo_f,bo_f,\n","                                     wc_f,uc_f,bc_f,\n","                                     Wattention_f,\n","                                     tf_seq_len,\n","                                     word_vec_dim)\n","    \n","    hidden_backward = backward_encoder(tf_text,\n","                                     initial_hidden_b,cell_b,\n","                                     wf_b,uf_b,bf_b,\n","                                     wi_b,ui_b,bi_b,\n","                                     wo_b,uo_b,bo_b,\n","                                     wc_b,uc_b,bc_b,\n","                                     Wattention_b,\n","                                     tf_seq_len,\n","                                     word_vec_dim)\n","    \n","    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n","    \n","    #ATTENTION MECHANISM AND DECODER\n","    \n","    decoded_hidden = encoded_hidden[0]\n","    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n","    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n","    tf_embd_limit = tf.convert_to_tensor(np_embd_limit)\n","    \n","    y = tf.convert_to_tensor(SOS) #inital decoder token <SOS> vector\n","    y = tf.reshape(y,[1,word_vec_dim])\n","    \n","    j=K\n","    \n","    hidden_residuals_stack = hidden_residuals_d.stack()\n","    \n","    RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n","    RRA = tf.reshape(RRA,[1,2*hidden_size])\n","    \n","    decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n","                                  wf_d,uf_d,bf_d,\n","                                  wi_d,ui_d,bf_d,\n","                                  wo_d,uo_d,bf_d,\n","                                  wc_d,uc_d,bc_d,\n","                                  RRA)\n","    decoded_hidden = decoded_hidden_next\n","    \n","    hidden_residuals_d = hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size]))\n","    \n","    j=j+1\n","                           \n","    i=0\n","    \n","    def attention_decoder_cond(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n","        return i < tf_output_len\n","    \n","    def attention_decoder_body(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):     \n","        #LOCAL ATTENTION\n","        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n","        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n","        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n","        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n","        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n","        \n","        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n","        \n","        #DECODER\n","        \n","        y = tf.matmul(attended_hidden,Ws)\n","        \n","        output = output.write(i,tf.reshape(y,[vocab_len]))\n","        #Save probability distribution as output\n","        \n","        y = tf.nn.softmax(y)\n","        \n","        y_index = tf.cast(tf.argmax(tf.reshape(y,[vocab_len])),tf.int32)\n","        y = tf_embd_limit[y_index]\n","        y = tf.reshape(y,[1,word_vec_dim])\n","        \n","        #setting next decoder input token as the word_vector of maximum probability \n","        #as found from previous attention-decoder output.\n","        \n","        hidden_residuals_stack = hidden_residuals_d.stack()\n","        \n","        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n","        RRA = tf.reshape(RRA,[1,2*hidden_size])\n","        \n","        decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n","                                  wf_d,uf_d,bf_d,\n","                                  wi_d,ui_d,bf_d,\n","                                  wo_d,uo_d,bf_d,\n","                                  wc_d,uc_d,bc_d,\n","                                  RRA)\n","        \n","        decoded_hidden = decoded_hidden_next\n","        \n","        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K+1), #(+1 for <SOS>)\n","                                   lambda: hidden_residuals_d,\n","                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n","        \n","        return i+1,j+1,decoded_hidden,cell_d,hidden_residuals_d,output\n","    \n","    i,j,decoded_hidden,cell_d,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n","                                            attention_decoder_body,\n","                                            [i,j,decoded_hidden,cell_d,hidden_residuals_d,output])\n","    hidden_residuals_d.close().mark_used()\n","    \n","    output = output.stack()\n","    \n","    return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0GR1BgZu7J52","colab_type":"code","outputId":"1a39ac9b-34c0-47ca-ffc2-3f3c848b8409","executionInfo":{"status":"ok","timestamp":1552840754639,"user_tz":0,"elapsed":4158,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"cell_type":"code","source":["output = model(tf_text,tf_seq_len,tf_output_len)\n","\n","#OPTIMIZER\n","\n","cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","#PREDICTION\n","\n","pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n","\n","i=0\n","\n","def cond_pred(i,pred):\n","    return i<tf_output_len\n","def body_pred(i,pred):\n","    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n","    return i+1,pred\n","\n","i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n","\n","prediction = pred.stack()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]},{"metadata":{"id":"pciYh5z37J54","colab_type":"code","outputId":"92fbd8a1-92bc-4416-cca1-b2b9183fe3ae","executionInfo":{"status":"error","timestamp":1552838966071,"user_tz":0,"elapsed":29164,"user":{"displayName":"Chang LIU","photoUrl":"https://lh4.googleusercontent.com/-sAiBk7MO_qk/AAAAAAAAAAI/AAAAAAAAAGc/nwwOGV6a7Yg/s64/photo.jpg","userId":"13266213205235766669"}},"colab":{"base_uri":"https://localhost:8080/","height":14113}},"cell_type":"code","source":["import string\n","from __future__ import print_function\n","\n","init = tf.global_variables_initializer()\n","\n","\n","with tf.Session() as sess: # Start Tensorflow Session\n","    \n","    saver = tf.train.Saver() \n","    # Prepares variable for saving the model\n","    sess.run(init) #initialize all variables\n","    step = 0   \n","    loss_list=[]\n","    acc_list=[]\n","    val_loss_list=[]\n","    val_acc_list=[]\n","    best_val_acc=0\n","    display_step = 1000\n","    \n","    while step < training_iters:\n","        \n","        total_loss=0\n","        total_acc=0\n","        total_val_loss = 0\n","        total_val_acc = 0\n","           \n","        for i in xrange(0,train_len):\n","            \n","            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n","            \n","            if i%display_step==0:\n","                print(\"\\nIteration: \"+str(i))\n","                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n","                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n","            \n","                print(\"\\nTEXT:\")\n","                flag = 0\n","                for vec in train_texts[i]:\n","                    if vec2word(vec) in string.punctuation or flag==0:\n","                        print(str(vec2word(vec)),end='')\n","                    else:\n","                        print((\" \"+str(vec2word(vec))),end='')\n","                    flag=1\n","\n","                print(\"\\n\")\n","\n","\n","            # Run optimization operation (backpropagation)\n","            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n","                                                    tf_seq_len: len(train_texts[i]), \n","                                                    tf_summary: train_out,\n","                                                    tf_output_len: len(train_out)})\n","            \n","         \n","            if i%display_step==0:\n","                saver.save(sess, '/content/gdrive/My Drive/MSc ML/0087/model.ckpt')\n","                print(\"\\nPREDICTED SUMMARY:\\n\")\n","                flag = 0\n","                for index in pred:\n","                    #if int(index)!=vocab_limit.index('eos'):\n","                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n","                        print(str(vocab_limit[int(index)]),end='')\n","                    else:\n","                        print(\" \"+str(vocab_limit[int(index)]),end='')\n","                    flag=1\n","                print(\"\\n\")\n","                \n","                print(\"ACTUAL SUMMARY:\\n\")\n","                flag = 0\n","                for vec in train_summaries[i]:\n","                    if vec2word(vec)!='eos':\n","                        if vec2word(vec) in string.punctuation or flag==0:\n","                            print(str(vec2word(vec)),end='')\n","                        else:\n","                            print((\" \"+str(vec2word(vec))),end='')\n","                    flag=1\n","\n","                print(\"\\n\")\n","                print(\"loss=\"+str(loss))\n","\n","\n","        step=step+1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Iteration: 0\n","Training input sequence length: 33\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","my daughter wanted this book and the price on amazon was the best. she has already tried one recipe a day after receiving the book. she seems happy with it.\n","\n","\n","PREDICTED SUMMARY:\n","\n","lucidity wannabe\n","\n","ACTUAL SUMMARY:\n","\n","best price\n","\n","loss=10.763502\n","\n","Iteration: 1000\n","Training input sequence length: 73\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","my son was recently diagnosed as having food sensitivities to wheat and especially to unk. not having mac and cheese( his favorite food) was especially hard. we tried some other products, but the general consensus was they tasted like cardboard. when we tried this the whole family loved it. it does n't really taste like mac and cheese, but tastes great non the less.\n","\n","\n","PREDICTED SUMMARY:\n","\n","need eat eat eat\n","\n","ACTUAL SUMMARY:\n","\n","say cheese- great\n","\n","loss=21.42155\n","\n","Iteration: 2000\n","Training input sequence length: 22\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","could n't find these anywhere but on line- it was worth the wait because they are unk great tasting cookies.\n","\n","\n","PREDICTED SUMMARY:\n","\n","yum this\n","\n","ACTUAL SUMMARY:\n","\n","wonderful cookies\n","\n","loss=22.798416\n","\n","Iteration: 3000\n","Training input sequence length: 71\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i just purchased this one, and after 1st use, cut my finger. `` why? '' might you ask, because the area is too small that you peel with. it is inset between the parts that hold the blade. this might be good for carrots, but if you have a large potato or one with a flat side, it is very ineffective.\n","\n","\n","PREDICTED SUMMARY:\n","\n","new new new grain\n","\n","ACTUAL SUMMARY:\n","\n","cutting area too small\n","\n","loss=32.904446\n","\n","Iteration: 4000\n","Training input sequence length: 68\n","Training target outputs sequence length: 8\n","\n","TEXT:\n","i used this for several years, and it is fine for what it is. i like the handle, but the head is a bit small, which can tear the unk 've since replaced it with the unk version, which i like a lot better: oxo good grips unk unk meat unk this review was helpful, please mark it as such.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great kitchen great.....\n","\n","ACTUAL SUMMARY:\n","\n","okay, but they make a better one\n","\n","loss=18.582975\n","\n","Iteration: 5000\n","Training input sequence length: 80\n","Training target outputs sequence length: 6\n","\n","TEXT:\n","i ordered this flaxseed after reading how good ground flaxseed was for you. i ordered a grinder, both were reasonably priced, and the grinding was so easily accomplished. i have been using flaxseed daily since that time, and the claims made on the internet have proven to be true. the product arrived on time and in great shape -- i have purchased from this vendor several times, and have not been disappointed.\n","\n","\n","PREDICTED SUMMARY:\n","\n","popcorn seasoning seasoning seasoning no no\n","\n","ACTUAL SUMMARY:\n","\n","so good for you!!\n","\n","loss=17.93902\n","\n","Iteration: 6000\n","Training input sequence length: 73\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i use this in my unk but make it with warm milk and an additional egg, which greatly improves the texture. when made with just water and a small amount of butter, the crust comes out very, very chewy in my unk. i have also made this bread and used carrot juice in place of the water, which makes it a beautiful color and naturally sweet.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great unk coffee coffee\n","\n","ACTUAL SUMMARY:\n","\n","can be improved easily\n","\n","loss=35.70669\n","\n","Iteration: 7000\n","Training input sequence length: 67\n","Training target outputs sequence length: 3\n","\n","TEXT:\n","if you have something that can still pay vhs and are into the oldies girl groups, buy this now. listen. i have hundreds of dollars of these kinds of videos and dvd 's. i am an oldies fanatic! and i love the old footage! this video has a bunch of super stuff!! it is totally worth the money.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great 's mom\n","\n","ACTUAL SUMMARY:\n","\n","girl groups vhs\n","\n","loss=54.69087\n","\n","Iteration: 8000\n","Training input sequence length: 54\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","i like the whole steel series of utensils and this pork fits nicely with the bunch. it 's comfortable to hold, easy to clean in the dish washer and does its job quietly and without fuss just like the rest of the tools in the series. ca n't go wrong.\n","\n","\n","PREDICTED SUMMARY:\n","\n","iffy blue\n","\n","ACTUAL SUMMARY:\n","\n","nice fork\n","\n","loss=31.336317\n","\n","Iteration: 9000\n","Training input sequence length: 30\n","Training target outputs sequence length: 5\n","\n","TEXT:\n","emeril 's big easy bold is my favorite. it is full flavored and robust yet smooth, and never bitter. shopping online is hassle free and convenient!\n","\n","\n","PREDICTED SUMMARY:\n","\n","good coffee palabras penny un\n","\n","ACTUAL SUMMARY:\n","\n","emeril 's big easy bold\n","\n","loss=22.827557\n","\n","Iteration: 10000\n","Training input sequence length: 77\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i like them as a midmorning snack with a cup of coffee. they are also great for those mornings you really are n't in the mood for breakfast, but know you 'll be hungry later. they seem to keep you satisfied for an hour or two. they do taste great- not too sweet and not overly unk. buy a bunch and keep them in the freezer until you need more.\n","\n","\n","PREDICTED SUMMARY:\n","\n","love as functional amber\n","\n","ACTUAL SUMMARY:\n","\n","great mid morning snack\n","\n","loss=37.4748\n","\n","Iteration: 11000\n","Training input sequence length: 26\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","these are the best chocolate fudge cookies commercially available. and the price is way below what you would have to pay at the grocers.\n","\n","\n","PREDICTED SUMMARY:\n","\n","spread favorite\n","\n","ACTUAL SUMMARY:\n","\n","chocolate fudge\n","\n","loss=34.710487\n","\n","Iteration: 12000\n","Training input sequence length: 26\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","green mountain unk are consistently a great value. breakfast blend is n't too strong or too weak, just right for my whole family.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great set set set\n","\n","ACTUAL SUMMARY:\n","\n","great unk and price\n","\n","loss=6.4268622\n","\n","Iteration: 13000\n","Training input sequence length: 61\n","Training target outputs sequence length: 3\n","\n","TEXT:\n","i tried the black cherry version and liked it but this one is much better. great orange taste plus the benefit of juice. and no corn syrup unk for those worried about its use. the carbonation helps make it a satisfying drink to quench thirst and the knowledge that it is healthy satisfies the mind as well.\n","\n","\n","PREDICTED SUMMARY:\n","\n","perfect percolator tenors\n","\n","ACTUAL SUMMARY:\n","\n","super satisfying taste\n","\n","loss=26.955437\n","\n","Iteration: 14000\n","Training input sequence length: 43\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","slips easily into bag& is a great snack unk. but be aware, it contains two servings& plan accordingly. love the `` auto ship ''& it gets cheaper the longer you stay with it! good deal!\n","\n","\n","PREDICTED SUMMARY:\n","\n","excellent kitchen price price\n","\n","ACTUAL SUMMARY:\n","\n","yummy& portable!\n","\n","loss=19.311863\n","\n","Iteration: 15000\n","Training input sequence length: 36\n","Training target outputs sequence length: 8\n","\n","TEXT:\n","was quite pleased with the product. arrived on time and wrapped well. would buy again and again!!!!! try it, you 'll let it!!!!!\n","\n","\n","PREDICTED SUMMARY:\n","\n","a!!!!!!!\n","\n","ACTUAL SUMMARY:\n","\n","great pastry buy!!!!!\n","\n","loss=9.210456\n","\n","Iteration: 16000\n","Training input sequence length: 36\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","the coffee is good, however the unk is not packaged well, the coffee machine barely streams coffee, more of a fast drip. the whole idea is it 's fast, right?\n","\n","\n","PREDICTED SUMMARY:\n","\n","best flavor coffee live\n","\n","ACTUAL SUMMARY:\n","\n","good coffee bad packaging\n","\n","loss=9.595463\n","\n","Iteration: 0\n","Training input sequence length: 33\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","my daughter wanted this book and the price on amazon was the best. she has already tried one recipe a day after receiving the book. she seems happy with it.\n","\n","\n","PREDICTED SUMMARY:\n","\n","perfect lover\n","\n","ACTUAL SUMMARY:\n","\n","best price\n","\n","loss=19.035843\n","\n","Iteration: 1000\n","Training input sequence length: 73\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","my son was recently diagnosed as having food sensitivities to wheat and especially to unk. not having mac and cheese( his favorite food) was especially hard. we tried some other products, but the general consensus was they tasted like cardboard. when we tried this the whole family loved it. it does n't really taste like mac and cheese, but tastes great non the less.\n","\n","\n","PREDICTED SUMMARY:\n","\n","wonderful deal&&\n","\n","ACTUAL SUMMARY:\n","\n","say cheese- great\n","\n","loss=22.00423\n","\n","Iteration: 2000\n","Training input sequence length: 22\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","could n't find these anywhere but on line- it was worth the wait because they are unk great tasting cookies.\n","\n","\n","PREDICTED SUMMARY:\n","\n","nice product\n","\n","ACTUAL SUMMARY:\n","\n","wonderful cookies\n","\n","loss=25.367188\n","\n","Iteration: 3000\n","Training input sequence length: 71\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i just purchased this one, and after 1st use, cut my finger. `` why? '' might you ask, because the area is too small that you peel with. it is inset between the parts that hold the blade. this might be good for carrots, but if you have a large potato or one with a flat side, it is very ineffective.\n","\n","\n","PREDICTED SUMMARY:\n","\n","good for good huge\n","\n","ACTUAL SUMMARY:\n","\n","cutting area too small\n","\n","loss=39.822815\n","\n","Iteration: 4000\n","Training input sequence length: 68\n","Training target outputs sequence length: 8\n","\n","TEXT:\n","i used this for several years, and it is fine for what it is. i like the handle, but the head is a bit small, which can tear the unk 've since replaced it with the unk version, which i like a lot better: oxo good grips unk unk meat unk this review was helpful, please mark it as such.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great tenderizer! option can can can can\n","\n","ACTUAL SUMMARY:\n","\n","okay, but they make a better one\n","\n","loss=20.878199\n","\n","Iteration: 5000\n","Training input sequence length: 80\n","Training target outputs sequence length: 6\n","\n","TEXT:\n","i ordered this flaxseed after reading how good ground flaxseed was for you. i ordered a grinder, both were reasonably priced, and the grinding was so easily accomplished. i have been using flaxseed daily since that time, and the claims made on the internet have proven to be true. the product arrived on time and in great shape -- i have purchased from this vendor several times, and have not been disappointed.\n","\n","\n","PREDICTED SUMMARY:\n","\n","cheesy seasoning risk risk cheesy.\n","\n","ACTUAL SUMMARY:\n","\n","so good for you!!\n","\n","loss=18.535498\n","\n","Iteration: 6000\n","Training input sequence length: 73\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i use this in my unk but make it with warm milk and an additional egg, which greatly improves the texture. when made with just water and a small amount of butter, the crust comes out very, very chewy in my unk. i have also made this bread and used carrot juice in place of the water, which makes it a beautiful color and naturally sweet.\n","\n","\n","PREDICTED SUMMARY:\n","\n","does good kick kick\n","\n","ACTUAL SUMMARY:\n","\n","can be improved easily\n","\n","loss=35.431755\n","\n","Iteration: 7000\n","Training input sequence length: 67\n","Training target outputs sequence length: 3\n","\n","TEXT:\n","if you have something that can still pay vhs and are into the oldies girl groups, buy this now. listen. i have hundreds of dollars of these kinds of videos and dvd 's. i am an oldies fanatic! and i love the old footage! this video has a bunch of super stuff!! it is totally worth the money.\n","\n","\n","PREDICTED SUMMARY:\n","\n","twinings good twinings\n","\n","ACTUAL SUMMARY:\n","\n","girl groups vhs\n","\n","loss=50.661907\n","\n","Iteration: 8000\n","Training input sequence length: 54\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","i like the whole steel series of utensils and this pork fits nicely with the bunch. it 's comfortable to hold, easy to clean in the dish washer and does its job quietly and without fuss just like the rest of the tools in the series. ca n't go wrong.\n","\n","\n","PREDICTED SUMMARY:\n","\n","beats good\n","\n","ACTUAL SUMMARY:\n","\n","nice fork\n","\n","loss=30.430939\n","\n","Iteration: 9000\n","Training input sequence length: 30\n","Training target outputs sequence length: 5\n","\n","TEXT:\n","emeril 's big easy bold is my favorite. it is full flavored and robust yet smooth, and never bitter. shopping online is hassle free and convenient!\n","\n","\n","PREDICTED SUMMARY:\n","\n","lovin with penny penny penny\n","\n","ACTUAL SUMMARY:\n","\n","emeril 's big easy bold\n","\n","loss=18.675709\n","\n","Iteration: 10000\n","Training input sequence length: 77\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i like them as a midmorning snack with a cup of coffee. they are also great for those mornings you really are n't in the mood for breakfast, but know you 'll be hungry later. they seem to keep you satisfied for an hour or two. they do taste great- not too sweet and not overly unk. buy a bunch and keep them in the freezer until you need more.\n","\n","\n","PREDICTED SUMMARY:\n","\n","the them tv tv\n","\n","ACTUAL SUMMARY:\n","\n","great mid morning snack\n","\n","loss=41.80446\n","\n","Iteration: 11000\n","Training input sequence length: 26\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","these are the best chocolate fudge cookies commercially available. and the price is way below what you would have to pay at the grocers.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great like\n","\n","ACTUAL SUMMARY:\n","\n","chocolate fudge\n","\n","loss=49.582947\n","\n","Iteration: 12000\n","Training input sequence length: 26\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","green mountain unk are consistently a great value. breakfast blend is n't too strong or too weak, just right for my whole family.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great loves i and\n","\n","ACTUAL SUMMARY:\n","\n","great unk and price\n","\n","loss=4.990153\n","\n","Iteration: 13000\n","Training input sequence length: 61\n","Training target outputs sequence length: 3\n","\n","TEXT:\n","i tried the black cherry version and liked it but this one is much better. great orange taste plus the benefit of juice. and no corn syrup unk for those worried about its use. the carbonation helps make it a satisfying drink to quench thirst and the knowledge that it is healthy satisfies the mind as well.\n","\n","\n","PREDICTED SUMMARY:\n","\n","love percolator percolator\n","\n","ACTUAL SUMMARY:\n","\n","super satisfying taste\n","\n","loss=31.083384\n","\n","Iteration: 14000\n","Training input sequence length: 43\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","slips easily into bag& is a great snack unk. but be aware, it contains two servings& plan accordingly. love the `` auto ship ''& it gets cheaper the longer you stay with it! good deal!\n","\n","\n","PREDICTED SUMMARY:\n","\n","no best dandy no\n","\n","ACTUAL SUMMARY:\n","\n","yummy& portable!\n","\n","loss=26.135689\n","\n","Iteration: 15000\n","Training input sequence length: 36\n","Training target outputs sequence length: 8\n","\n","TEXT:\n","was quite pleased with the product. arrived on time and wrapped well. would buy again and again!!!!! try it, you 'll let it!!!!!\n","\n","\n","PREDICTED SUMMARY:\n","\n","a tree a tea!!!!\n","\n","ACTUAL SUMMARY:\n","\n","great pastry buy!!!!!\n","\n","loss=10.194277\n","\n","Iteration: 16000\n","Training input sequence length: 36\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","the coffee is good, however the unk is not packaged well, the coffee machine barely streams coffee, more of a fast drip. the whole idea is it 's fast, right?\n","\n","\n","PREDICTED SUMMARY:\n","\n","what coffee container ever\n","\n","ACTUAL SUMMARY:\n","\n","good coffee bad packaging\n","\n","loss=20.239716\n","\n","Iteration: 0\n","Training input sequence length: 33\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","my daughter wanted this book and the price on amazon was the best. she has already tried one recipe a day after receiving the book. she seems happy with it.\n","\n","\n","PREDICTED SUMMARY:\n","\n","perfect lover\n","\n","ACTUAL SUMMARY:\n","\n","best price\n","\n","loss=21.085936\n","\n","Iteration: 1000\n","Training input sequence length: 73\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","my son was recently diagnosed as having food sensitivities to wheat and especially to unk. not having mac and cheese( his favorite food) was especially hard. we tried some other products, but the general consensus was they tasted like cardboard. when we tried this the whole family loved it. it does n't really taste like mac and cheese, but tastes great non the less.\n","\n","\n","PREDICTED SUMMARY:\n","\n","love pasta pictured pictured\n","\n","ACTUAL SUMMARY:\n","\n","say cheese- great\n","\n","loss=27.133648\n","\n","Iteration: 2000\n","Training input sequence length: 22\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","could n't find these anywhere but on line- it was worth the wait because they are unk great tasting cookies.\n","\n","\n","PREDICTED SUMMARY:\n","\n","nice vacuum\n","\n","ACTUAL SUMMARY:\n","\n","wonderful cookies\n","\n","loss=35.444904\n","\n","Iteration: 3000\n","Training input sequence length: 71\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i just purchased this one, and after 1st use, cut my finger. `` why? '' might you ask, because the area is too small that you peel with. it is inset between the parts that hold the blade. this might be good for carrots, but if you have a large potato or one with a flat side, it is very ineffective.\n","\n","\n","PREDICTED SUMMARY:\n","\n","bargain peeler pro pro\n","\n","ACTUAL SUMMARY:\n","\n","cutting area too small\n","\n","loss=38.180992\n","\n","Iteration: 4000\n","Training input sequence length: 68\n","Training target outputs sequence length: 8\n","\n","TEXT:\n","i used this for several years, and it is fine for what it is. i like the handle, but the head is a bit small, which can tear the unk 've since replaced it with the unk version, which i like a lot better: oxo good grips unk unk meat unk this review was helpful, please mark it as such.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great really. really really really really really\n","\n","ACTUAL SUMMARY:\n","\n","okay, but they make a better one\n","\n","loss=26.765003\n","\n","Iteration: 5000\n","Training input sequence length: 80\n","Training target outputs sequence length: 6\n","\n","TEXT:\n","i ordered this flaxseed after reading how good ground flaxseed was for you. i ordered a grinder, both were reasonably priced, and the grinding was so easily accomplished. i have been using flaxseed daily since that time, and the claims made on the internet have proven to be true. the product arrived on time and in great shape -- i have purchased from this vendor several times, and have not been disappointed.\n","\n","\n","PREDICTED SUMMARY:\n","\n","popcorn great tongs tongs tongs tongs\n","\n","ACTUAL SUMMARY:\n","\n","so good for you!!\n","\n","loss=16.303667\n","\n","Iteration: 6000\n","Training input sequence length: 73\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i use this in my unk but make it with warm milk and an additional egg, which greatly improves the texture. when made with just water and a small amount of butter, the crust comes out very, very chewy in my unk. i have also made this bread and used carrot juice in place of the water, which makes it a beautiful color and naturally sweet.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great worst what coffee\n","\n","ACTUAL SUMMARY:\n","\n","can be improved easily\n","\n","loss=39.493484\n","\n","Iteration: 7000\n","Training input sequence length: 67\n","Training target outputs sequence length: 3\n","\n","TEXT:\n","if you have something that can still pay vhs and are into the oldies girl groups, buy this now. listen. i have hundreds of dollars of these kinds of videos and dvd 's. i am an oldies fanatic! and i love the old footage! this video has a bunch of super stuff!! it is totally worth the money.\n","\n","\n","PREDICTED SUMMARY:\n","\n","great& mom\n","\n","ACTUAL SUMMARY:\n","\n","girl groups vhs\n","\n","loss=69.336205\n","\n","Iteration: 8000\n","Training input sequence length: 54\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","i like the whole steel series of utensils and this pork fits nicely with the bunch. it 's comfortable to hold, easy to clean in the dish washer and does its job quietly and without fuss just like the rest of the tools in the series. ca n't go wrong.\n","\n","\n","PREDICTED SUMMARY:\n","\n","nice in\n","\n","ACTUAL SUMMARY:\n","\n","nice fork\n","\n","loss=29.411165\n","\n","Iteration: 9000\n","Training input sequence length: 30\n","Training target outputs sequence length: 5\n","\n","TEXT:\n","emeril 's big easy bold is my favorite. it is full flavored and robust yet smooth, and never bitter. shopping online is hassle free and convenient!\n","\n","\n","PREDICTED SUMMARY:\n","\n","oco good a expensive expensive\n","\n","ACTUAL SUMMARY:\n","\n","emeril 's big easy bold\n","\n","loss=19.821218\n","\n","Iteration: 10000\n","Training input sequence length: 77\n","Training target outputs sequence length: 4\n","\n","TEXT:\n","i like them as a midmorning snack with a cup of coffee. they are also great for those mornings you really are n't in the mood for breakfast, but know you 'll be hungry later. they seem to keep you satisfied for an hour or two. they do taste great- not too sweet and not overly unk. buy a bunch and keep them in the freezer until you need more.\n","\n","\n","PREDICTED SUMMARY:\n","\n","maple!! syrup\n","\n","ACTUAL SUMMARY:\n","\n","great mid morning snack\n","\n","loss=36.09817\n","\n","Iteration: 11000\n","Training input sequence length: 26\n","Training target outputs sequence length: 2\n","\n","TEXT:\n","these are the best chocolate fudge cookies commercially available. and the price is way below what you would have to pay at the grocers.\n","\n","\n","PREDICTED SUMMARY:\n","\n","inexpensive favorite\n","\n","ACTUAL SUMMARY:\n","\n","chocolate fudge\n","\n","loss=39.911125\n"],"name":"stdout"}]},{"metadata":{"id":"PjWJMOH57J57","colab_type":"text"},"cell_type":"markdown","source":["### To Try\\ To Do\\ To keep in mind:\n","\n","* Beam Search\n","* Pointer Mechanisms\n","* Heirarchical attention\n","* [Intra-input-attention](https://arxiv.org/pdf/1705.04304.pdf)\n","* Better pre-processing\n","* Switch to PyTorch for dynamic models.\n","* Mini-Batch Training\n","* Better Datasets.\n","* Train for different tasks (eg. Translation) using different datasets.\n","* Intra-layer attention for both encoder and decoder together with everything else.\n","* Adopt a more object oriented approach"]},{"metadata":{"id":"58Bh4sd27J58","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}